ID:
  name: first_run
data:
  dir: data
  sample_key: soma_joinid
  batch_key: dataset_id
  cell_type_key: cell_type
  train:
    subdir: ${data.dir}/train_data
    filenames: [subset_large_cellxgene.h5ad]
    out_dir: ${data.dir}/train_data_processed_${ID.name}
  test:
    subdir: ${data.dir}/test_data
    filenames: [small_cellxgene.h5ad]
    out_dir: ${data.dir}/test_data_processed_${ID.name}
pp: #preprocessing
  general:
    train_size: 0.8
    # Consolidate or remove the following categories if they have less samples than the treshhold
    categories: ["cell_type", "dataset_id"]
    threshold: 5
    remove: True
  embedder:
    context_embedder:
      use_precalculated: False
      precalculated_obsm_key: c_emb
      type: categorial
    category_embedder:
      metadata_categories: ["cell_type"]
      one_hot: False
      model: text-embedding-3-small
      combination_method: concatenate
      embeddings_file_path: ${data.dir}/emb_dicts/category_embeddings_{.model}_metadata_embeddings.pkl.gz
    data_embedder:
      use_precalculated: True
      precalculated_obsm_key: scvi
      type: None
  normalizer:
    type: None #min-max or z-score or None
  aligner:
    latent_dim: 64
    type: pca
    pca_eval:
      evaluate_pca: False
      save_path: pca_eval
      scree_plot: True
      cumulative_variance_plot: True
      loadings_heatmap: True
      loadings_heatmap_options:
        threshold: 0 # Only include loadings above this threshold
        top_n_components: 64 # Number of principal components to include in the heatmap
        top_n_variables: 100 # Number of variables to display in the heatmap
  dataset_constructor:
    seq_length: 12
    batch_size: 32
engine:
  model:
    type: mmcontext_encoder #mmcontext_encoder
    latent_dim: ${pp.aligner.latent_dim}
    hidden_dim: 64
    num_layers: 2
    num_heads: 1
    use_self_attention: False
    use_cross_attention: False
    activation: relu
    dropout: 0.1
  losses:
    - type: contrastive_loss
      use: True
      weight: 1.0
      target_mode: infoNCE
      current_mode: data_context
      similarity_metric: cosine

    - type: contrastive_loss
      use: True
      weight: 0.5
      target_mode: context_context
      current_mode: data_data
      similarity_metric: euclidean

    - type: reconstruction_loss
      use: False
      weight: 1.0
      reduction: mean

  optimizer:
    - type: adam
      use: True
      lr: 0.001
      weight_decay: 0.0
      betas: [0.9, 0.999]
      max_lr: None

    - type: adam
      use: False
      lr: 0.01
      weight_decay: 0.1
      betas: [0.9, 0.999]
      max_lr: None

  scheduler: # Currently schedulers will be applied after each epoch
    - type: step
      use: True
      step_size: 10
      gamma: 0.1

    - type: cosine
      use: False
      T_max: 10
      eta_min: 0.0001

  Trainer:
    input_embeddings: #This determines which embeddings are used as input to the model. If you changed the default names of the embeddings, you need to change them here
      main: data_embedding
      cross: context_embedding
    temperature: null # This relates to cosine similarity loss and will be learned if None, or can be set to fixed value
    epochs: 100
    save_path: best_model.pth
