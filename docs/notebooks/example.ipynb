{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: context aware learning of multiple modalities with mmcontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the mmcontext package to preprocess single-cell data. We'll walk through the steps of:\n",
    "\n",
    "**Preprocessing**:\n",
    "1. Loading the Dataset\n",
    "2. Generating Embeddings\n",
    "3. Normalizing Embeddings\n",
    "4. Aligning Embeddings\n",
    "5. Constructing the Dataset\n",
    "6. Creating a Data Loader\n",
    "\n",
    "**Model fitting**:\n",
    "1. Initialize the model\n",
    "2. Configure the loss\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### 1. Loading the Dataset\n",
    "\n",
    "The input data has to be an {class}`anndata.AnnData` object. First, we'll load the example dataset provided in data/small_cellxgene.h5ad. It is derived from the cellxgene census {cite:p}`czi2023cz` and contains cells of various tissues and celltypes from different studys. The scvi embedding included is provided by cellxgene and contains embeddings computed with the scvi variational autoencoder trained on the cellxgene corpus. It is a custom made dataset that contains data from various tissues originating from different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 5600 × 1000\n",
      "    obs: 'soma_joinid', 'donor_id', 'disease', 'sex', 'dataset_id', 'cell_type', 'assay', 'tissue', 'cell_type_ontology_term_id', 'assay_ontology_term_id', 'tissue_ontology_term_id', 'suspension_type', 'is_primary_data'\n",
      "    var: 'soma_joinid', 'feature_id', 'feature_name', 'feature_length', 'nnz', 'n_measured_obs'\n",
      "    obsm: 'metadata_tissue_assay_cell_type', 'scvi'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import anndata\n",
    "\n",
    "data_path = \"../../data/test_data/small_cellxgene.h5ad\"\n",
    "# Load the dataset\n",
    "adata = anndata.read_h5ad(data_path)\n",
    "\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Embeddings\n",
    "\n",
    "We will generate context embeddings based on the categorical metadata fields cell_type and tissue using the {class}`mmcontext.pp.CategoryEmbedder` class.\n",
    "The method is based on creating an embedding of the individual categories of a cell with a text model. The `embeddings_file_path` points to a dictionary that contains embeddings for a range of cell types and tissues from the cellxgene corpus, thereby allowing the method to work without needing an API call. Only if some categories are not found in the dictionary the api will be used. If only a few are unknown, these will just be filled with a zero embedding. The `unkown_threshold` parameter controls how many new categories are needed to use the API. For that of course an API Key will be needed, which has to be set as an environmental variable \"OPENAI_API_KEY\". \n",
    "\n",
    "We will use the precomputed data embeddings stored in `adata.obsm['scvi']` as our data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.pp.context_embedder - INFO - Loaded embeddings from file.\n",
      "mmcontext.pp.context_embedder - INFO - Embeddings dictionary contains the following categories: dict_keys(['cell_type', 'tissue', 'assay']) with a total of 947 elements.\n",
      "mmcontext.pp.embedder - INFO - Using external data embeddings provided.\n",
      "mmcontext.pp.embedder - INFO - Creating context embeddings...\n",
      "mmcontext.pp.context_embedder - INFO - Embeddings for 'cell_type' stored in adata.obsm['cell_type_emb']\n",
      "mmcontext.pp.context_embedder - INFO - Embeddings for 'tissue' stored in adata.obsm['tissue_emb']\n",
      "mmcontext.pp.context_embedder - INFO - Combined context embeddings stored in adata.obsm['c_emb']\n",
      "Context Embeddings Shape: (5600, 1536)\n",
      "Data Embeddings Shape: (5600, 50)\n"
     ]
    }
   ],
   "source": [
    "# Import the CategoryEmbedder class\n",
    "from mmcontext.pp import CategoryEmbedder, Embedder\n",
    "\n",
    "# Specify the categories to embed\n",
    "categories = [\"cell_type\", \"tissue\"]\n",
    "\n",
    "# Initialize the CategoryEmbedder\n",
    "category_embedder = CategoryEmbedder(\n",
    "    metadata_categories=categories,\n",
    "    model=\"text-embedding-3-small\",\n",
    "    combination_method=\"average\",\n",
    "    embeddings_file_path=\"../../data/emb_dicts/category_embeddings_text-embedding-3-small_metadata_embeddings.pkl.gz\",\n",
    ")\n",
    "# Initialize the Embedder without embedders\n",
    "embedder = Embedder(context_embedder=category_embedder, data_embedder=None)\n",
    "\n",
    "# Create embeddings using external embeddings\n",
    "embedder.create_embeddings(adata, data_embeddings=adata.obsm[\"scvi\"])\n",
    "\n",
    "# Confirm the shape of the context embeddings\n",
    "print(\"Context Embeddings Shape:\", adata.obsm[\"c_emb\"].shape)\n",
    "print(\"Data Embeddings Shape:\", adata.obsm[\"d_emb\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize Embeddings \n",
    "\n",
    "Now that the embeddings are created and stored in the adata object we can apply normalization. We will use the {class}`mmcontext.pp.MinMaxNormalizer` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.pp.embedding_normalizer - INFO - Normalizing embeddings using min-max normalization...\n",
      "Normalized Data Embeddings Shape: (5600, 50)\n",
      "Normalized Context Embeddings Shape: (5600, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Import the MinMaxNormalizer class\n",
    "from mmcontext.pp import MinMaxNormalizer\n",
    "\n",
    "# Initialize the MinMaxNormalizer\n",
    "normalizer = MinMaxNormalizer()\n",
    "# Normalize the embeddings\n",
    "normalizer.normalize(adata)\n",
    "\n",
    "# Confirm that normalized embeddings are stored\n",
    "print(\"Normalized Data Embeddings Shape:\", adata.obsm[\"d_emb_norm\"].shape)\n",
    "print(\"Normalized Context Embeddings Shape:\", adata.obsm[\"c_emb_norm\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Aligning Embeddings\n",
    "\n",
    "After normalization we will now use a {class}`mmcontext.pp.DimAligner` to make sure the dimensions of both data and context embeddings are equal, as this will be nescessary for the model. We will use the {class}`mmcontext.pp.PCAReducer` for this. If embeddings are larger than target latent dimension, they will be reduced via PCA. If there are smaller, padding with zeros will be applied. If using the {class}`mmcontext.pp.PCAReducer`, you can evaluate the principal components with some plots and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary for PCA evaluation\n",
    "config = {\n",
    "    \"evaluate_pca\": True,\n",
    "    \"save_path\": \"pca_eval\",\n",
    "    \"scree_plot\": True,\n",
    "    \"cumulative_variance_plot\": True,\n",
    "    \"loadings_heatmap\": True,\n",
    "    \"loadings_heatmap_options\": {\n",
    "        \"threshold\": 0.05,  # Only include loadings above this threshold\n",
    "        \"top_n_components\": 50,  # Number of principal components to include in the heatmap\n",
    "        \"top_n_variables\": 100,  # Number of variables to display in the heatmap\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.pp.dim_aligner - INFO - Kaiser Criterion: Number of components with eigenvalue > 1: 14\n",
      "mmcontext.pp.dim_aligner - ERROR - Error computing KMO measure: SVD did not converge\n",
      "Aligned Data Embeddings Shape: (5600, 64)\n",
      "Aligned Context Embeddings Shape: (5600, 64)\n"
     ]
    }
   ],
   "source": [
    "# Import the PCAReducer class\n",
    "from mmcontext.pp import PCAReducer\n",
    "\n",
    "# Initialize the PCAReducer with the desired latent dimension\n",
    "latent_dim = 64\n",
    "aligner = PCAReducer(latent_dim=latent_dim, config=config)\n",
    "\n",
    "# Align the embeddings\n",
    "aligner.align(adata)\n",
    "\n",
    "# Confirm that aligned embeddings are stored\n",
    "print(\"Aligned Data Embeddings Shape:\", adata.obsm[\"d_emb_aligned\"].shape)\n",
    "print(\"Aligned Context Embeddings Shape:\", adata.obsm[\"c_emb_aligned\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Constructing the Dataset\n",
    "\n",
    "Finally, we will construct a dataset using the aligned embeddings, suitable for training models with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DataSetConstructor class\n",
    "from torch.utils.data import random_split\n",
    "from mmcontext.pp import DataSetConstructor\n",
    "\n",
    "# Initialize the DataSetConstructor\n",
    "dataset_constructor = DataSetConstructor(sample_id_key=\"soma_joinid\")\n",
    "\n",
    "# Add the AnnData object to the dataset\n",
    "dataset_constructor.add_anndata(adata)\n",
    "\n",
    "# The sequence length for the dataset. Since attention mechanisms will be used, samples can be grouped into sequences to perform attention within the sequence.\n",
    "seq_length = 20\n",
    "# Construct the dataset\n",
    "dataset = dataset_constructor.construct_dataset(seq_length=seq_length)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a Dataloader\n",
    "Create a pyTorch Dataloader which can be iterated over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 64])\n",
      "torch.Size([4, 20, 64])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 4  # Number of sequences per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over DataLoader and verify batch shapes\n",
    "for batch in train_loader:\n",
    "    data_embeddings = batch[\"data_embedding\"]  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "    context_embeddings = batch[\"context_embedding\"]  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "    sample_ids = batch[\"sample_id\"]  # Shape: (batch_size, seq_length)\n",
    "\n",
    "    print(data_embeddings.shape)\n",
    "    print(context_embeddings.shape)\n",
    "    print(sample_ids.shape)\n",
    "    break  # Only need to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "#### 1. Initializing the Model\n",
    "The {class} `mmcontext.engine.MMContextEncoder` is the main encoder which will be used to create embeddings based on the available data and context embeddings. It's structure is flexible and is build upon torchs {class} `torch.nn.TransformerEncoder` which creates stacks of the {class} `mmcontext.engine.CustomTransformerEncoderLayer` which can be configured to be \n",
    "\n",
    "1. An MLP only model\n",
    "2. To apply self attention (use_self_attention = True)\n",
    "3. To apply cross attention (use_cross_attention = True)\n",
    "4. To use both self and cross attention (both True)\n",
    "\n",
    "The model takes two matrix inputs, `in_main` and `in_cross`. in_main will be passed through the MLP and optionally the self-attention layers, while in_cross is only used if cross-attention is used. In the end the model outputs embeddings of the same shape as in_main, updated based on the learning objective. The inputs are handled in the {class} `mmcontext.engine.Trainer` in dependancy with the settings used for the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.engine.models - INFO - MMContextEncoder initialized with embedding_dim = 64, num_layers = 4, use_self_attention = True, use_cross_attention = True.\n"
     ]
    }
   ],
   "source": [
    "from mmcontext.engine import MMContextEncoder\n",
    "\n",
    "model = MMContextEncoder(\n",
    "    embedding_dim=latent_dim,  # this has to be the same dimension as the latent dimension of the aligner\n",
    "    hidden_dim=32,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    use_self_attention=True,\n",
    "    use_cross_attention=True,\n",
    "    activation=\"relu\",\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Configure the loss\n",
    "The loss function is a central part of this project. The loss is implemented to be extendable and customizable by having a {class} `mmcontext.engine.LossManager` which you can use to add different losses to and which will be passed to the {class} `mmcontext.engine.Trainer` to compute the total loss (as a weighted average of the components) during Training. The current main implementation of a {class} `mmcontext.engine.LossFunction` is the {class} `mmcontext.engine.ContrastiveLoss`, which is a custom approach to contrastive learning. It's main configuration parameters are `target_mode` and `current_mode` which refer to the way in which the target similarity matrix and the current (during model training) similarity matrix are constructed. For example if `target_mode = 'context_context'`, the original context embeddings are used to create the target similarity matrix. Therefore during training, the loss is calcualted as the mean squared error between the current similarity matrix and the one based on the context. If `current_mode = 'data_data'`, the model would learn to find representations of the data that represent similarity found in the respective context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.engine.losses - INFO - Adding loss function: ContrastiveLoss(target_mode=context_context, current_mode=data_data, similarity_metric=cosine) with weight 1.00\n",
      "mmcontext.engine.losses - INFO - Adding loss function: ContrastiveLoss(target_mode=context_context, current_mode=context_data, similarity_metric=cosine) with weight 1.00\n"
     ]
    }
   ],
   "source": [
    "from mmcontext.engine import LossManager, ContrastiveLoss\n",
    "\n",
    "loss_manager = LossManager()\n",
    "loss_manager.add_loss(\n",
    "    ContrastiveLoss(target_mode=\"context_context\", current_mode=\"data_data\", similarity_metric=\"cosine\")\n",
    ")\n",
    "loss_manager.add_loss(\n",
    "    ContrastiveLoss(target_mode=\"context_context\", current_mode=\"context_data\", similarity_metric=\"cosine\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Model\n",
    "The Trainer uses the defined model and loss the conduct training, aiming to iterativly minimize the loss. The {func} `mmcontext.engine.Trainer.fit` method can take a training and a validation dataloader as input. If a validation loader is given and a save_path is used, the weights of the best performing model can be saved to file. The {class} `mmcontext.engine.MMContextEncoder` has a method `load` to load weights from file.\n",
    "Per default data embeddings are used for in_main while context embeddings are used as in_cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmcontext.engine.trainer - INFO - Starting Epoch 1/5\n",
      "mmcontext.engine.trainer - INFO - Batch 10/56, Loss: 0.2461\n",
      "mmcontext.engine.trainer - INFO - Batch 20/56, Loss: 0.3424\n",
      "mmcontext.engine.trainer - INFO - Batch 30/56, Loss: 0.1636\n",
      "mmcontext.engine.trainer - INFO - Batch 40/56, Loss: 0.1348\n",
      "mmcontext.engine.trainer - INFO - Batch 50/56, Loss: 0.1172\n",
      "mmcontext.engine.trainer - INFO - Batch 56/56, Loss: 0.1386\n",
      "mmcontext.engine.trainer - INFO - Training Epoch Complete. Average Loss: 0.2189\n",
      "mmcontext.engine.trainer - INFO - Validation Complete. Average Loss: 0.1345\n",
      "mmcontext.engine.trainer - INFO - Epoch 1/5 - Train Loss: 0.2189, Val Loss: 0.1345\n",
      "mmcontext.engine.trainer - INFO - Starting Epoch 2/5\n",
      "mmcontext.engine.trainer - INFO - Batch 10/56, Loss: 0.0761\n",
      "mmcontext.engine.trainer - INFO - Batch 20/56, Loss: 0.0952\n",
      "mmcontext.engine.trainer - INFO - Batch 30/56, Loss: 0.1005\n",
      "mmcontext.engine.trainer - INFO - Batch 40/56, Loss: 0.0654\n",
      "mmcontext.engine.trainer - INFO - Batch 50/56, Loss: 0.0686\n",
      "mmcontext.engine.trainer - INFO - Batch 56/56, Loss: 0.0724\n",
      "mmcontext.engine.trainer - INFO - Training Epoch Complete. Average Loss: 0.1034\n",
      "mmcontext.engine.trainer - INFO - Validation Complete. Average Loss: 0.0883\n",
      "mmcontext.engine.trainer - INFO - Epoch 2/5 - Train Loss: 0.1034, Val Loss: 0.0883\n",
      "mmcontext.engine.trainer - INFO - Starting Epoch 3/5\n",
      "mmcontext.engine.trainer - INFO - Batch 10/56, Loss: 0.0512\n",
      "mmcontext.engine.trainer - INFO - Batch 20/56, Loss: 0.0548\n",
      "mmcontext.engine.trainer - INFO - Batch 30/56, Loss: 0.0863\n",
      "mmcontext.engine.trainer - INFO - Batch 40/56, Loss: 0.0571\n",
      "mmcontext.engine.trainer - INFO - Batch 50/56, Loss: 0.0550\n",
      "mmcontext.engine.trainer - INFO - Batch 56/56, Loss: 0.0497\n",
      "mmcontext.engine.trainer - INFO - Training Epoch Complete. Average Loss: 0.0770\n",
      "mmcontext.engine.trainer - INFO - Validation Complete. Average Loss: 0.0794\n",
      "mmcontext.engine.trainer - INFO - Epoch 3/5 - Train Loss: 0.0770, Val Loss: 0.0794\n",
      "mmcontext.engine.trainer - INFO - Starting Epoch 4/5\n",
      "mmcontext.engine.trainer - INFO - Batch 10/56, Loss: 0.0387\n",
      "mmcontext.engine.trainer - INFO - Batch 20/56, Loss: 0.0591\n",
      "mmcontext.engine.trainer - INFO - Batch 30/56, Loss: 0.0750\n",
      "mmcontext.engine.trainer - INFO - Batch 40/56, Loss: 0.0515\n",
      "mmcontext.engine.trainer - INFO - Batch 50/56, Loss: 0.0477\n",
      "mmcontext.engine.trainer - INFO - Batch 56/56, Loss: 0.0418\n",
      "mmcontext.engine.trainer - INFO - Training Epoch Complete. Average Loss: 0.0681\n",
      "mmcontext.engine.trainer - INFO - Validation Complete. Average Loss: 0.0645\n",
      "mmcontext.engine.trainer - INFO - Epoch 4/5 - Train Loss: 0.0681, Val Loss: 0.0645\n",
      "mmcontext.engine.trainer - INFO - Starting Epoch 5/5\n",
      "mmcontext.engine.trainer - INFO - Batch 10/56, Loss: 0.0373\n",
      "mmcontext.engine.trainer - INFO - Batch 20/56, Loss: 0.0430\n",
      "mmcontext.engine.trainer - INFO - Batch 30/56, Loss: 0.0630\n",
      "mmcontext.engine.trainer - INFO - Batch 40/56, Loss: 0.0429\n",
      "mmcontext.engine.trainer - INFO - Batch 50/56, Loss: 0.0503\n",
      "mmcontext.engine.trainer - INFO - Batch 56/56, Loss: 0.0389\n",
      "mmcontext.engine.trainer - INFO - Training Epoch Complete. Average Loss: 0.0607\n",
      "mmcontext.engine.trainer - INFO - Validation Complete. Average Loss: 0.0609\n",
      "mmcontext.engine.trainer - INFO - Epoch 5/5 - Train Loss: 0.0607, Val Loss: 0.0609\n"
     ]
    }
   ],
   "source": [
    "from mmcontext.engine import Trainer\n",
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    loss_manager=loss_manager,\n",
    "    optimizer=torch.optim.Adam(model.parameters()),\n",
    "    input_embeddings={\"main\": \"data_embedding\", \"cross\": \"context_embedding\"},\n",
    ")\n",
    "trainer.fit(train_loader, val_loader, epochs=5)\n",
    "\n",
    "# The fitted model can be used to create universial embeddings\n",
    "modified_embeddings = trainer.infer(val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmcontext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
