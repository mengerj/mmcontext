{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# model = transformers.AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_model_cfg = {\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 4,\n",
    "    \"use_self_attention\": False,\n",
    "    \"activation\": \"relu\",\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.models import MMContextEncoder\n",
    "\n",
    "model = MMContextEncoder(\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    omics_processor_name=\"none\",\n",
    "    omics_encoder_cfg=omics_model_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "modules = [model]\n",
    "bimodal_model = SentenceTransformer(modules=modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal_model.save(\"../../../data/models/mmcontext_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"../../../data/models/mmcontext_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers.dataclass import omicsSample\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def simulate_omics_dataset(\n",
    "    num_samples: int,\n",
    "    num_features: int,\n",
    "    mean: float = 0.0,\n",
    "    std_dev: float = 1.0,\n",
    "    zero_fraction: float = 0.5,  # Fraction of values to set to zero\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Simulates a dataset of omicsSample instances, with random zeros in counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        Number of samples to simulate.\n",
    "    num_features : int\n",
    "        Number of features per sample.\n",
    "    mean : float, optional\n",
    "        Mean of the normal distribution used to generate counts, default is 0.0.\n",
    "    std_dev : float, optional\n",
    "        Standard deviation of the normal distribution used to generate counts, default is 1.0.\n",
    "    zero_fraction : float, optional\n",
    "        Fraction of the counts to randomly set to zero, default is 0.1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[omicsSample]\n",
    "        A list of simulated omicsSample instances.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        # Generate random counts from a normal distribution\n",
    "        counts = np.random.normal(loc=mean, scale=std_dev, size=num_features)\n",
    "        counts = np.abs(counts)  # Make all counts positive\n",
    "        # Randomly set some values to zero\n",
    "        num_zeros = int(zero_fraction * num_features)\n",
    "        zero_indices = np.random.choice(range(num_features), size=num_zeros, replace=False)\n",
    "        counts[zero_indices] = 0\n",
    "\n",
    "        # Generate random feature IDs\n",
    "        featureIDs = [f\"{random.randint(10000, 99999)}\" for _ in range(num_features)]\n",
    "\n",
    "        # Create a mapping from feature ID to index\n",
    "        feature_to_idx = {feature_id: idx for idx, feature_id in enumerate(featureIDs)}\n",
    "\n",
    "        # Create an omicsSample instance\n",
    "        sample = {\n",
    "            \"counts\": counts,\n",
    "            \"featureIDs\": featureIDs,\n",
    "            \"feature_to_idx\": feature_to_idx,\n",
    "            \"sample_id\": str(sample_id),\n",
    "            \"is_omics\": True,\n",
    "        }\n",
    "\n",
    "        dataset.append(sample)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.27828777e-01, -1.15590954e+00,  1.05809890e-01,\n",
       "         1.28279865e+00,  4.61147904e-01, -1.90415710e-01,\n",
       "        -1.07769382e+00, -7.64065146e-01, -1.16255488e-02,\n",
       "        -6.28843963e-01, -1.49026692e+00, -8.07196319e-01,\n",
       "        -2.64787376e-01,  2.47096324e+00,  2.00408340e+00,\n",
       "        -9.25594747e-01, -3.15756679e-01, -6.21650279e-01,\n",
       "        -7.46325791e-01, -4.23527420e-01, -4.95181233e-01,\n",
       "         1.22440822e-01, -7.53941596e-01, -1.19791830e+00,\n",
       "        -1.58664241e-01, -1.39167786e+00,  2.17830229e+00,\n",
       "        -1.06094408e+00,  3.27068120e-01,  1.35503501e-01,\n",
       "        -3.46453160e-01,  2.45105553e+00, -5.24826586e-01,\n",
       "        -5.78065217e-01,  2.00095668e-01, -8.13486993e-01,\n",
       "         2.14783698e-01, -5.67257524e-01,  1.22807288e+00,\n",
       "         5.68531632e-01, -3.15219849e-01, -7.14145839e-01,\n",
       "        -7.98902035e-01, -5.05987227e-01,  1.91066980e+00,\n",
       "        -4.39112276e-01,  4.58433598e-01,  1.34201753e+00,\n",
       "        -5.36090076e-01,  2.69287825e+00, -7.41880596e-01,\n",
       "        -6.17172778e-01, -1.00200081e+00,  2.43203923e-01,\n",
       "        -4.70097601e-01, -2.26372927e-01,  3.94157737e-01,\n",
       "        -5.54235697e-01, -2.38836497e-01,  1.40549660e+00,\n",
       "        -4.45736647e-01,  8.60976130e-02,  1.84366897e-01,\n",
       "         1.82205856e+00],\n",
       "       [-3.51038247e-01, -4.65703577e-01, -1.12616926e-01,\n",
       "         1.67533422e+00,  1.24847329e+00, -7.56710649e-01,\n",
       "        -2.49775708e-01,  2.09135818e+00, -4.58287776e-01,\n",
       "        -3.26371282e-01, -1.29611015e+00, -4.86938506e-01,\n",
       "        -6.48370981e-01, -3.17761183e-01, -5.72305024e-01,\n",
       "        -1.07867038e+00, -1.68760315e-01, -3.01826417e-01,\n",
       "        -8.12533617e-01, -5.83925664e-01,  1.59420729e-01,\n",
       "         1.15253401e+00, -8.15602064e-01, -1.13509154e+00,\n",
       "        -4.62743938e-01, -1.62947857e+00,  1.25280249e+00,\n",
       "         7.66243041e-01, -5.97064495e-01, -6.36058748e-01,\n",
       "        -5.44615507e-01, -1.00410736e+00,  2.48357892e+00,\n",
       "         1.03455281e+00, -6.09285295e-01,  6.41267955e-01,\n",
       "        -3.57588410e-01,  1.89181447e+00, -8.20944011e-01,\n",
       "         6.62672818e-02,  7.75048183e-03, -8.79397750e-01,\n",
       "        -1.89518571e-01,  3.12713802e-01, -2.31489465e-01,\n",
       "         4.05940190e-02, -1.00059462e+00, -4.24165726e-02,\n",
       "        -1.26224542e+00,  3.33847356e+00,  1.32182715e-04,\n",
       "        -1.19732656e-01, -1.13208473e+00, -3.76968116e-01,\n",
       "         1.93360531e+00, -7.52389431e-02,  9.63927388e-01,\n",
       "         7.99674571e-01, -8.12479019e-01,  2.85397619e-01,\n",
       "        -9.36832964e-01,  1.16687238e+00,  5.77004701e-02,\n",
       "         1.28879654e+00],\n",
       "       [ 2.30844289e-01,  2.09293365e+00, -6.32601559e-01,\n",
       "         1.71091303e-01,  1.04703093e+00, -3.67026925e-01,\n",
       "        -8.70068610e-01, -8.27517986e-01, -3.84891421e-01,\n",
       "         1.97708106e+00, -1.47311676e+00, -2.63575286e-01,\n",
       "        -9.44390297e-01, -8.41136277e-01, -6.16536319e-01,\n",
       "         1.27005517e+00, -6.08473681e-02, -5.65482564e-02,\n",
       "         1.36966670e+00,  3.72250021e-01, -1.64357781e-01,\n",
       "        -1.06316745e+00,  8.78881812e-01,  1.38889706e+00,\n",
       "        -6.67299747e-01, -3.81559402e-01, -1.65636182e-01,\n",
       "        -7.19166040e-01, -3.70947063e-01,  1.20825633e-01,\n",
       "         8.39542508e-01, -4.27610874e-01,  1.07671177e+00,\n",
       "         3.52631092e-01, -7.11904824e-01, -5.31449616e-01,\n",
       "         3.86722893e-01, -1.33008087e+00,  2.64458847e+00,\n",
       "        -3.78908634e-01, -5.36538720e-01,  6.51838779e-01,\n",
       "        -7.89868653e-01, -1.00020695e+00, -7.02815473e-01,\n",
       "        -9.22598839e-01, -1.52985021e-01,  1.28468966e+00,\n",
       "        -3.69883478e-01, -5.21274090e-01, -7.75482357e-01,\n",
       "        -3.17949414e-01, -6.51865900e-01,  1.66586375e+00,\n",
       "        -4.19629604e-01, -1.26513079e-01, -2.72861093e-01,\n",
       "         2.09978914e+00,  7.30696082e-01, -8.67520452e-01,\n",
       "        -1.33741164e+00, -1.54040933e-01, -1.33870530e+00,\n",
       "         2.85586452e+00],\n",
       "       [ 7.02123165e-01, -1.08493459e+00,  9.46826935e-01,\n",
       "        -1.06970227e+00, -6.72501504e-01,  4.27434534e-01,\n",
       "        -1.23730993e+00,  9.48472440e-01, -1.40606725e+00,\n",
       "         2.86515534e-01,  2.24470353e+00,  1.59140694e+00,\n",
       "        -3.54313493e-01,  1.65554619e+00,  1.62554085e+00,\n",
       "        -5.79796195e-01, -3.00188214e-01,  1.59806162e-01,\n",
       "        -8.02098274e-01,  9.39617231e-02,  2.03837454e-01,\n",
       "        -1.30816802e-01, -5.41514814e-01,  3.43911499e-01,\n",
       "        -9.43918884e-01, -9.10924613e-01,  5.34733176e-01,\n",
       "        -1.40700722e+00, -6.97539270e-01, -1.12820280e+00,\n",
       "         1.45454156e+00,  1.33815873e+00, -5.60635328e-01,\n",
       "        -4.34045233e-02,  1.74416676e-01, -1.55292541e-01,\n",
       "         3.21167946e+00,  3.10042053e-01, -4.54701513e-01,\n",
       "         1.58407104e+00,  5.51430523e-01, -1.48411381e+00,\n",
       "        -2.22887844e-01, -6.08667135e-01,  6.21248960e-01,\n",
       "        -5.37165701e-01,  1.01494014e+00, -4.82978523e-01,\n",
       "        -6.95759773e-01, -6.89218700e-01, -5.93465447e-01,\n",
       "        -6.78860128e-01, -8.33163917e-01, -6.42420292e-01,\n",
       "         8.39573368e-02, -9.75788057e-01,  1.88729930e+00,\n",
       "         3.24180424e-02, -5.21542072e-01,  4.44414556e-01,\n",
       "         1.50879943e+00, -4.75992382e-01, -1.05281866e+00,\n",
       "        -1.00652528e+00]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "num_samples = 5\n",
    "num_features = 64\n",
    "simulated_omicsSamples = simulate_omics_dataset(num_samples, num_features)\n",
    "\n",
    "model.encode(\n",
    "    [simulated_omicsSamples[0], simulated_omicsSamples[1], simulated_omicsSamples[2], simulated_omicsSamples[3]],\n",
    "    batch_size=2,\n",
    "    device=\"mps\",\n",
    ")\n",
    "# model.encode([\"This is a whole sentence. Maybe it is long, or not.\",\"This is a another sentence.\",\"one more\",\"and another one\"], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers.dataclass import omicsSample\n",
    "class InputExample:\n",
    "    \"\"\"Structure for one input example with texts, the label and a unique id\"\"\"\n",
    "\n",
    "    def __init__(self, guid: str = \"\", texts: list[dict, str] = None, label: int | float = 0):\n",
    "        \"\"\"\n",
    "        Creates one InputExample with the given texts, guid and label\n",
    "\n",
    "        Args:\n",
    "            guid: id for the example\n",
    "            texts: the texts for the example.\n",
    "            label: the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_dataset = []\n",
    "for idx in range(0, len(simulated_omicsSamples) - 1, 2):\n",
    "    # We can use image pairs directly. Because our images aren't labeled, we use a random label as an example\n",
    "    # train_dataset.append(InputExample(texts=[photos[idx], photos[idx + 1]], label=random.choice([0, 1])))\n",
    "\n",
    "    # Or images and text together\n",
    "    # simulated_dataset.append(InputExample(texts=[simulated_omicsSamples[idx], simulated_omicsSamples[idx+1]], label=1))\n",
    "    simulated_dataset.append(\n",
    "        InputExample(texts=[simulated_omicsSamples[idx], \"This is another unrelated caption\"], label=0)\n",
    "    )\n",
    "    simulated_dataset.append(InputExample(texts=[simulated_omicsSamples[idx], \"This is a related caption\"], label=1))\n",
    "    # simulated_dataset.append(InputExample(texts=[\"This is just text\", \"this is related text\"], label = 1))\n",
    "    # simulated_dataset.append(InputExample(texts=[\"This is just text\", \"this is unrelated text\"], label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a DataLoader that batches our data and prepare a contrastive loss function\n",
    "from sentence_transformers import losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(simulated_dataset, shuffle=True, batch_size=4)\n",
    "train_loss = losses.ContrastiveLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "  0%|          | 0/5 [03:13<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:20<00:00,  2.82s/it]\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 20.4788, 'train_samples_per_second': 0.977, 'train_steps_per_second': 0.244, 'train_loss': 0.21001834869384767, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit([(train_dataloader, train_loss)], epochs=5, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulated_omicsSamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis is a similar caption\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:103\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcos_sim\u001b[39m(a: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor, b: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity between two tensors.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        Tensor: Matrix with res[i][j] = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_batch_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     b \u001b[38;5;241m=\u001b[39m _convert_to_batch_tensor(b)\n\u001b[1;32m    106\u001b[0m     a_norm \u001b[38;5;241m=\u001b[39m normalize_embeddings(a)\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:73\u001b[0m, in \u001b[0;36m_convert_to_batch_tensor\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_to_batch_tensor\u001b[39m(a: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Converts the input data to a tensor with a batch dimension.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        Tensor: The converted tensor with a batch dimension.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     a \u001b[38;5;241m=\u001b[39m _convert_to_batch(a)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:44\u001b[0m, in \u001b[0;36m_convert_to_tensor\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mConverts the input `a` to a PyTorch tensor if it is not already a tensor.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Tensor: The converted tensor.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, Tensor):\n\u001b[0;32m---> 44\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "model.similarity(simulated_omicsSamples[0], \"This is a similar caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils.import_utils import is_accelerate_available\n",
    "\n",
    "is_accelerate_available(\"0.28.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils.import_utils import _accelerate_available, _accelerate_version\n",
    "\n",
    "print(_accelerate_available)  # Should be True if the package is installed\n",
    "print(_accelerate_version)  # Should print the version string, e.g., \"0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, '1.3.0')\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils.import_utils import _is_package_available\n",
    "\n",
    "print(_is_package_available(\"accelerate\", return_version=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformers.utils.import_utils' from '/Users/mengerj/repos/sentence-transformers/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py'>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import transformers.utils.import_utils\n",
    "\n",
    "importlib.reload(transformers.utils.import_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mengerj/repos/sentence-transformers/.venv/bin/python\n",
      "['/Users/mengerj/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python312.zip', '/Users/mengerj/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12', '/Users/mengerj/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/lib-dynload', '', '/Users/mengerj/repos/sentence-transformers/.venv/lib/python3.12/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)  # Path to the current Python interpreter\n",
    "print(sys.path)  # Directories being searched for packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "\n",
    "def _is_package_available(pkg_name: str, return_version: bool = False) -> tuple[bool, str] | bool:\n",
    "    # Check if the package spec exists and grab its version to avoid importing a local directory\n",
    "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
    "    package_version = \"N/A\"\n",
    "    if package_exists:\n",
    "        try:\n",
    "            # Primary method to get the package version\n",
    "            package_version = importlib.metadata.version(pkg_name)\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            # Fallback method: Only for \"torch\" and versions containing \"dev\"\n",
    "            if pkg_name == \"torch\":\n",
    "                try:\n",
    "                    package = importlib.import_module(pkg_name)\n",
    "                    temp_version = getattr(package, \"__version__\", \"N/A\")\n",
    "                    # Check if the version contains \"dev\"\n",
    "                    if \"dev\" in temp_version:\n",
    "                        package_version = temp_version\n",
    "                        package_exists = True\n",
    "                    else:\n",
    "                        package_exists = False\n",
    "                except ImportError:\n",
    "                    # If the package can't be imported, it's not available\n",
    "                    package_exists = False\n",
    "            else:\n",
    "                # For packages other than \"torch\", don't attempt the fallback and set as not available\n",
    "                package_exists = False\n",
    "        print(f\"Detected {pkg_name} version: {package_version}\")\n",
    "    if return_version:\n",
    "        return package_exists, package_version\n",
    "    else:\n",
    "        return package_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models\n",
    "from PIL import Image\n",
    "\n",
    "clip = models.CLIPModel()\n",
    "model = SentenceTransformer(modules=[clip])\n",
    "# Encode an image:\n",
    "img_emb = model.encode(Image.open(\"two_dogs_in_snow.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = Image.open(\"two_dogs_in_snow.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "# We can use image pairs directly. Because our images aren't labeled, we use a random label as an example\n",
    "# train_dataset.append(InputExample(texts=[photos[idx], photos[idx + 1]], label=random.choice([0, 1])))\n",
    "\n",
    "# Or images and text together\n",
    "train_dataset.append(InputExample(texts=[photo, \"This is the caption\"], label=1))\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
    "train_loss = losses.ContrastiveLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [05:52<?, ?it/s]                                \n",
      "100%|██████████| 5/5 [00:00<00:00,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.788, 'train_samples_per_second': 6.345, 'train_steps_per_second': 6.345, 'train_loss': 0.3352612018585205, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit([(train_dataloader, train_loss)], epochs=5, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of JpegImageFile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphoto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis is a similar caption\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:103\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcos_sim\u001b[39m(a: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor, b: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity between two tensors.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        Tensor: Matrix with res[i][j] = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_batch_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     b \u001b[38;5;241m=\u001b[39m _convert_to_batch_tensor(b)\n\u001b[1;32m    106\u001b[0m     a_norm \u001b[38;5;241m=\u001b[39m normalize_embeddings(a)\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:73\u001b[0m, in \u001b[0;36m_convert_to_batch_tensor\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_to_batch_tensor\u001b[39m(a: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Converts the input data to a tensor with a batch dimension.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        Tensor: The converted tensor with a batch dimension.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     a \u001b[38;5;241m=\u001b[39m _convert_to_batch(a)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m~/repos/sentence-transformers/sentence_transformers/util.py:44\u001b[0m, in \u001b[0;36m_convert_to_tensor\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mConverts the input `a` to a PyTorch tensor if it is not already a tensor.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Tensor: The converted tensor.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, Tensor):\n\u001b[0;32m---> 44\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of JpegImageFile"
     ]
    }
   ],
   "source": [
    "model.similarity(photo, \"This is a similar caption\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
