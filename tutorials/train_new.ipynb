{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641d7ea1",
   "metadata": {},
   "source": [
    "## Train a new mmcontext model (demonstrated on proteomics data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c71fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c910e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmcontext.utils import setup_logging\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eccf0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 10:06:20,161 - mmcontext.file_utils - INFO - File is a valid .h5ad file.\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "\n",
    "from mmcontext.file_utils import download_file_from_share_link\n",
    "\n",
    "# a protein dataset from figshare (https://plus.figshare.com/articles/dataset/scPerturb_Single-Cell_Perturbation_Data_RNA_and_protein_h5ad_files/24160713?utm_source=chatgpt.com&file=42428325)\n",
    "data_link = \"https://plus.figshare.com/ndownloader/files/42428325\"\n",
    "local_path = \"Frangiehlzar2021_protein.h5ad\"\n",
    "# download the data\n",
    "download_file_from_share_link(share_link=data_link, save_path=local_path)\n",
    "# load the data\n",
    "adata = ad.read_h5ad(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72302131",
   "metadata": {},
   "source": [
    "Of course one can create a more sophisicated description, this is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f28a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description string for each cell by looping over the rows in adata.obs\n",
    "def make_description(row):\n",
    "    \"\"\"Make a quick description of a cell based on its metadata\"\"\"\n",
    "    return (\n",
    "        f\"The first perturbation is {row['perturbation']} \"\n",
    "        f\"and the second perturbation is {row['perturbation_2']}.\"\n",
    "        f\" The tissue is {row['tissue_type']} and it has cancer yes or no: {row['cancer']}.\"\n",
    "        f\" The disease is {row['disease']}.\"\n",
    "        f\" The celltype is {row['celltype']}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Add a \"description\" column to adata.obs using the function above\n",
    "adata.obs[\"description\"] = adata.obs.apply(make_description, axis=1)\n",
    "# Also add a sample index column for later\n",
    "adata.obs[\"sample_idx\"] = adata.obs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d999832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's split by train and val, randomly 80% train\n",
    "adata.obs[\"split\"] = np.random.rand(len(adata)) < 0.8\n",
    "adata_train = adata[adata.obs[\"split\"]].copy()\n",
    "adata_val = adata[~adata.obs[\"split\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de1a9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43894, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68558ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174437, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffb5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:82: UserWarning: Some cells have zero counts\n",
      "  return fn(*args_all, **kw)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# normalise and log transform the data\n",
    "sc.pp.normalize_total(adata_train, inplace=True)\n",
    "sc.pp.log1p(adata_train)\n",
    "\n",
    "sc.pp.normalize_total(adata_val, inplace=True)\n",
    "sc.pp.log1p(adata_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b699d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'description' as categorical\n",
      "... storing 'description' as categorical\n"
     ]
    }
   ],
   "source": [
    "# since these datasets only contain 24 proteins, we will just use their expression as embeddings\n",
    "# we will use the protein names as the embedding keys\n",
    "adata_train.obsm[\"X_prot\"] = adata_train.X\n",
    "adata_val.obsm[\"X_prot\"] = adata_val.X\n",
    "processed_paths = {\"train\": \"Frangiehlzar2021_protein_pp_train.h5ad\", \"val\": \"Frangiehlzar2021_protein_pp_val.h5ad\"}\n",
    "adata_train.write_h5ad(processed_paths[\"train\"])\n",
    "adata_val.write_h5ad(processed_paths[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd012cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:09,802 - adata_hf_datasets.dataset.ds_constructor - INFO - AnnData ingested (174437 rows).\n",
      "2025-11-14 15:45:09,805 - adata_hf_datasets.dataset.ds_constructor - INFO - Building dataset from generator...\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb32fca325445dbb62f91cefac664ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:11,849 - adata_hf_datasets.dataset.ds_constructor - INFO - Constructed dataset with 174437 records in 'multiplets' format.\n",
      "2025-11-14 15:45:12,031 - adata_hf_datasets.dataset.ds_constructor - INFO - AnnData ingested (43894 rows).\n",
      "2025-11-14 15:45:12,032 - adata_hf_datasets.dataset.ds_constructor - INFO - Building dataset from generator...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371579c30cc44fa6bb9ffb6895a56808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:12,620 - adata_hf_datasets.dataset.ds_constructor - INFO - Constructed dataset with 43894 records in 'multiplets' format.\n"
     ]
    }
   ],
   "source": [
    "from adata_hf_datasets import AnnDataSetConstructor\n",
    "from datasets import DatasetDict\n",
    "\n",
    "ds_dict = DatasetDict()\n",
    "# multiplets format for training datasets (with descriptions)\n",
    "# A sentence key refers to the column in adata.obs that is used to represent the sample.\n",
    "# For numeric data, we use the sample index, and later register the created embedding linked to their indices in the tokenizer\n",
    "for split_name, adata_split in {\"train\": adata_train, \"val\": adata_val}.items():\n",
    "    constructor = AnnDataSetConstructor(dataset_format=\"multiplets\", resolve_negatives=True)\n",
    "    constructor.add_anndata(\n",
    "        adata_split,\n",
    "        caption_key=\"description\",\n",
    "        sentence_keys=[\"sample_idx\"],\n",
    "        adata_link=processed_paths[split_name],\n",
    "        batch_key=\"library_preparation_protocol\",  # In this case all are from the same batch, but providing a batch key can improve batch integration by negative sampling\n",
    "    )\n",
    "    ds = constructor.get_dataset()\n",
    "    ds_dict[split_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4d531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample_idx', 'cell_sentence_1', 'positive', 'negative_1_idx', 'adata_link', 'negative_1'],\n",
       "    num_rows: 43894\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70612269",
   "metadata": {},
   "source": [
    "## Configure the Model\n",
    "The custom sentence transformers model will allow training with the sentence transformers Trainer. The numeric data has to be registered with the model, such that the representations in cell_sentence_1 (cell token eg. sample indices) are linked to the respective numeric vector, which serves as the initial repesentation of that sample. \n",
    "This is achieved by building a lookup table (cell token -> id) and a frozen embedding layer (id --> numeric vector). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f04ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:13,529 - mmcontext.mmcontextencoder - INFO - Unfreezing last 2 layers of BERT-like model\n",
      "2025-11-14 15:45:13,530 - mmcontext.mmcontextencoder - INFO - Successfully unfroze 2 layers with 3548928 trainable parameters\n",
      "2025-11-14 15:45:13,560 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from mmcontext.mmcontextencoder import MMContextEncoder\n",
    "\n",
    "enc = MMContextEncoder(\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    adapter_hidden_dim=128,\n",
    "    adapter_output_dim=64,\n",
    "    freeze_text_encoder=True,\n",
    "    unfreeze_last_n_layers=2,\n",
    "    output_token_embeddings=False,\n",
    "    train_lookup=False,\n",
    "    joint_adapter_hidden_dim=None,\n",
    "    text_model_kwargs=None,\n",
    "    use_text_adapter=True,\n",
    ")\n",
    "model = SentenceTransformer(modules=[enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9a6e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:13,776 - mmcontext.file_utils - INFO - Found 2 unique share links across ['train', 'val'] splits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb1248262534f75be8c279abce72502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:45:13,779 - mmcontext.file_utils - INFO - Using local path: /Users/mengerj/repos/mmcontext/tutorials/Frangiehlzar2021_protein_pp_train.h5ad\n",
      "2025-11-14 15:45:13,779 - mmcontext.file_utils - INFO - Using local path: /Users/mengerj/repos/mmcontext/tutorials/Frangiehlzar2021_protein_pp_val.h5ad\n",
      "2025-11-14 15:45:13,837 - mmcontext.file_utils - INFO - Reading /Users/mengerj/repos/mmcontext/tutorials/Frangiehlzar2021_protein_pp_train.h5ad\n",
      "2025-11-14 15:45:15,130 - mmcontext.file_utils - INFO - Built DataFrame with 174437 rows × 24-dim embeddings\n",
      "2025-11-14 15:45:15,150 - mmcontext.file_utils - INFO - Reading /Users/mengerj/repos/mmcontext/tutorials/Frangiehlzar2021_protein_pp_val.h5ad\n",
      "2025-11-14 15:45:15,480 - mmcontext.file_utils - INFO - Built DataFrame with 43894 rows × 24-dim embeddings\n",
      "2025-11-14 15:45:15,483 - mmcontext.mmcontextencoder - INFO - Combined embedding DataFrame shape: (218331, 3)\n",
      "2025-11-14 15:45:15,703 - mmcontext.omicsencoder - INFO - Loaded embedding matrix with shape (218332, 24)\n",
      "2025-11-14 15:45:15,703 - mmcontext.mmcontextencoder - INFO - Registered 218332 new numeric samples (total 218332). ≈0.020 GiB added. (Assuming float32 precision.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the returned DataFrame to register the embeddings with `register_initial_embeddings()`.\n"
     ]
    }
   ],
   "source": [
    "token_df, _ = model[0].get_initial_embeddings_from_adata_link(\n",
    "    ds_dict,\n",
    "    layer_key=\"X_prot\",\n",
    "    download_dir=\"data_cache\",\n",
    "    axis=\"obs\",  # since we get embeddings from adata.obsm. We could also use \"varm\" and for example use an embedding for each protein\n",
    ")\n",
    "model[0].register_initial_embeddings(token_df, data_origin=\"prot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6975ab72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_idx:'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model expects a certain prefix on the cell tokens.\n",
    "model[0].processor.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc91911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667ce69ae6f6473aa3788c67f5878001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prefixing columns: ['cell_sentence_1']:   0%|          | 0/174437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3357184080404988bacc59e070d283b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prefixing columns: ['cell_sentence_1']:   0%|          | 0/43894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sample_idx', 'cell_sentence_1', 'positive', 'negative_1_idx', 'adata_link', 'negative_1'],\n",
       "        num_rows: 174437\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sample_idx', 'cell_sentence_1', 'positive', 'negative_1_idx', 'adata_link', 'negative_1'],\n",
       "        num_rows: 43894\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you could add this manually or use the method below\n",
    "model[0].prefix_ds(ds_dict, columns_to_prefix=[\"cell_sentence_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f63b0e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative_1'],\n",
       "        num_rows: 174437\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative_1'],\n",
       "        num_rows: 43894\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lastly we have to drop some columns from the dataset and\n",
    "# rename the main column to \"anchor\".\n",
    "# you might think that this is a bit cumbersome, which it is.\n",
    "# But this setup allowed for fleixble training,\n",
    "# using either cell or feature level tokens, using text based cell sentences\n",
    "# or numeric embeddings and resolving negatives\n",
    "# to whatever column was chosen for training.\n",
    "# That means that for a certain training run, the same dataset can be reused, and\n",
    "# only modified differently. But in the end, it is a bit of work to set up.\n",
    "ds_final = ds_dict.rename_column(\"cell_sentence_1\", \"anchor\")\n",
    "ds_final = ds_final.remove_columns([\"sample_idx\", \"adata_link\", \"negative_1_idx\"])\n",
    "ds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c0a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=10,\n",
    "    run_name=\"protein_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44a49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "evaluator = TripletEvaluator(\n",
    "    anchors=ds_final[\"val\"][\"anchor\"],\n",
    "    positives=ds_final[\"val\"][\"positive\"],\n",
    "    negatives=ds_final[\"val\"][\"negative_1\"],\n",
    "    name=\"val\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d32063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9038c393948c46a5b1ef4ca405f7d484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_final[\"train\"],\n",
    "    eval_dataset=ds_final[\"val\"],\n",
    "    loss=loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f83d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmengerj\u001b[0m (\u001b[33mmengerj-universit-tsklinikum-freiburg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mengerj/repos/mmcontext/tutorials/wandb/run-20251114_154541-ht0ph8j9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers/runs/ht0ph8j9' target=\"_blank\">protein_test</a></strong> to <a href='https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers' target=\"_blank\">https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers/runs/ht0ph8j9' target=\"_blank\">https://wandb.ai/mengerj-universit-tsklinikum-freiburg/sentence-transformers/runs/ht0ph8j9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1363' max='1363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1363/1363 25:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.715900</td>\n",
       "      <td>5.399902</td>\n",
       "      <td>0.778740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.649900</td>\n",
       "      <td>4.977130</td>\n",
       "      <td>0.857315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.345000</td>\n",
       "      <td>4.811664</td>\n",
       "      <td>0.857725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.264700</td>\n",
       "      <td>4.926792</td>\n",
       "      <td>0.785369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.159100</td>\n",
       "      <td>5.027446</td>\n",
       "      <td>0.737003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.141400</td>\n",
       "      <td>4.972731</td>\n",
       "      <td>0.742106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.139500</td>\n",
       "      <td>4.974823</td>\n",
       "      <td>0.736866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.075500</td>\n",
       "      <td>4.937728</td>\n",
       "      <td>0.729097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.073800</td>\n",
       "      <td>5.088612</td>\n",
       "      <td>0.674147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.044300</td>\n",
       "      <td>5.126331</td>\n",
       "      <td>0.662733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.049800</td>\n",
       "      <td>5.099945</td>\n",
       "      <td>0.664647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.039200</td>\n",
       "      <td>5.002823</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.037400</td>\n",
       "      <td>5.077823</td>\n",
       "      <td>0.672279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 15:46:23,048 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.07336757153338225 after 100 steps:\n",
      "2025-11-14 15:47:32,215 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t77.87%\n",
      "2025-11-14 15:47:32,217 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-100\n",
      "2025-11-14 15:47:32,217 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-100\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:48:17,890 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.1467351430667645 after 200 steps:\n",
      "2025-11-14 15:49:31,975 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t85.73%\n",
      "2025-11-14 15:49:31,978 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-200\n",
      "2025-11-14 15:49:31,978 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-200\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:50:42,675 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.22010271460014674 after 300 steps:\n",
      "2025-11-14 15:51:55,448 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t85.77%\n",
      "2025-11-14 15:51:55,450 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-300\n",
      "2025-11-14 15:51:55,450 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-300\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:52:37,051 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.293470286133529 after 400 steps:\n",
      "2025-11-14 15:53:47,271 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t78.54%\n",
      "2025-11-14 15:53:47,273 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-400\n",
      "2025-11-14 15:53:47,273 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-400\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:54:30,542 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.36683785766691124 after 500 steps:\n",
      "2025-11-14 15:55:38,895 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t73.70%\n",
      "2025-11-14 15:55:38,901 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-500\n",
      "2025-11-14 15:55:38,902 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-500\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:56:23,781 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.4402054292002935 after 600 steps:\n",
      "2025-11-14 15:57:33,509 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t74.21%\n",
      "2025-11-14 15:57:33,512 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-600\n",
      "2025-11-14 15:57:33,512 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-600\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 15:58:18,387 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.5135730007336757 after 700 steps:\n",
      "2025-11-14 15:59:22,144 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t73.69%\n",
      "2025-11-14 15:59:22,146 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-700\n",
      "2025-11-14 15:59:22,147 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-700\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:00:11,151 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.586940572267058 after 800 steps:\n",
      "2025-11-14 16:01:16,206 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t72.91%\n",
      "2025-11-14 16:01:16,208 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-800\n",
      "2025-11-14 16:01:16,208 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-800\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:02:02,905 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.6603081438004402 after 900 steps:\n",
      "2025-11-14 16:03:05,105 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t67.41%\n",
      "2025-11-14 16:03:05,107 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-900\n",
      "2025-11-14 16:03:05,107 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-900\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:03:56,006 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.7336757153338225 after 1000 steps:\n",
      "2025-11-14 16:04:57,668 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t66.27%\n",
      "2025-11-14 16:04:57,670 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-1000\n",
      "2025-11-14 16:04:57,671 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-1000\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:05:48,538 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.8070432868672047 after 1100 steps:\n",
      "2025-11-14 16:06:59,429 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t66.46%\n",
      "2025-11-14 16:06:59,431 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-1100\n",
      "2025-11-14 16:06:59,432 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-1100\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:07:46,415 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.880410858400587 after 1200 steps:\n",
      "2025-11-14 16:08:56,664 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t70.11%\n",
      "2025-11-14 16:08:56,666 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-1200\n",
      "2025-11-14 16:08:56,666 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-1200\n",
      "/Users/mengerj/repos/mmcontext/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-11-14 16:09:41,947 - sentence_transformers.evaluation.TripletEvaluator - INFO - TripletEvaluator: Evaluating the model on the val dataset in epoch 0.9537784299339692 after 1300 steps:\n",
      "2025-11-14 16:10:57,216 - sentence_transformers.evaluation.TripletEvaluator - INFO - Accuracy Cosine Similarity:\t67.23%\n",
      "2025-11-14 16:10:57,218 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-1300\n",
      "2025-11-14 16:10:57,218 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-1300\n",
      "2025-11-14 16:11:08,576 - sentence_transformers.trainer - INFO - Saving model checkpoint to trainer_output/checkpoint-1363\n",
      "2025-11-14 16:11:08,576 - sentence_transformers.SentenceTransformer - INFO - Save model to trainer_output/checkpoint-1363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1363, training_loss=4.516945848024163, metrics={'train_runtime': 1529.0258, 'train_samples_per_second': 114.084, 'train_steps_per_second': 0.891, 'total_flos': 0.0, 'train_loss': 4.516945848024163, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
