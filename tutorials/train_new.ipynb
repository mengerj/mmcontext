{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641d7ea1",
   "metadata": {},
   "source": [
    "## Train a new mmcontext model (demonstrated on proteomics data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c910e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcontext.utils import setup_logging\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "from mmcontext.file_utils import download_file_from_share_link\n",
    "\n",
    "# a protein dataset from figshare (https://plus.figshare.com/articles/dataset/scPerturb_Single-Cell_Perturbation_Data_RNA_and_protein_h5ad_files/24160713?utm_source=chatgpt.com&file=42428325)\n",
    "data_link = \"https://plus.figshare.com/ndownloader/files/42428325\"\n",
    "local_path = \"Frangiehlzar2021_protein.h5ad\"\n",
    "# download the data\n",
    "download_file_from_share_link(share_link=data_link, save_path=local_path)\n",
    "# load the data\n",
    "adata = ad.read_h5ad(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72302131",
   "metadata": {},
   "source": [
    "Of course one can create a more sophisicated description, this is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f28a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description string for each cell by looping over the rows in adata.obs\n",
    "def make_description(row):\n",
    "    \"\"\"Make a quick description of a cell based on its metadata\"\"\"\n",
    "    return (\n",
    "        f\"The first perturbation is {row['perturbation']} \"\n",
    "        f\"and the second perturbation is {row['perturbation_2']}.\"\n",
    "        f\" The tissue is {row['tissue_type']} and it has cancer yes or no: {row['cancer']}.\"\n",
    "        f\" The disease is {row['disease']}.\"\n",
    "        f\" The celltype is {row['celltype']}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Add a \"description\" column to adata.obs using the function above\n",
    "adata.obs[\"description\"] = adata.obs.apply(make_description, axis=1)\n",
    "# Also add a sample index column for later\n",
    "adata.obs[\"sample_idx\"] = adata.obs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d999832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's split by train and val, randomly 80% train\n",
    "adata.obs[\"split\"] = np.random.rand(len(adata)) < 0.8\n",
    "adata_train = adata[adata.obs[\"split\"]].copy()\n",
    "adata_val = adata[~adata.obs[\"split\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68558ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# normalise and log transform the data\n",
    "sc.pp.normalize_total(adata_train, inplace=True)\n",
    "sc.pp.log1p(adata_train)\n",
    "\n",
    "sc.pp.normalize_total(adata_val, inplace=True)\n",
    "sc.pp.log1p(adata_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since these datasets only contain 24 proteins, we will just use their expression as embeddings\n",
    "# we will use the protein names as the embedding keys\n",
    "adata_train.obsm[\"X_prot\"] = adata_train.X\n",
    "adata_val.obsm[\"X_prot\"] = adata_val.X\n",
    "processed_paths = {\"train\": \"Frangiehlzar2021_protein_pp_train.h5ad\", \"val\": \"Frangiehlzar2021_protein_pp_val.h5ad\"}\n",
    "adata_train.write_h5ad(processed_paths[\"train\"])\n",
    "adata_val.write_h5ad(processed_paths[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd012cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adata_hf_datasets import AnnDataSetConstructor\n",
    "from datasets import DatasetDict\n",
    "\n",
    "ds_dict = DatasetDict()\n",
    "# multiplets format for training datasets (with descriptions)\n",
    "# A sentence key refers to the column in adata.obs that is used to represent the sample.\n",
    "# For numeric data, we use the sample index, and later register the created embedding linked to their indices in the tokenizer\n",
    "for split_name, adata_split in {\"train\": adata_train, \"val\": adata_val}.items():\n",
    "    constructor = AnnDataSetConstructor(dataset_format=\"multiplets\", resolve_negatives=True)\n",
    "    constructor.add_anndata(\n",
    "        adata_split,\n",
    "        caption_key=\"description\",\n",
    "        sentence_keys=[\"sample_idx\"],\n",
    "        adata_link=processed_paths[split_name],\n",
    "        batch_key=\"library_preparation_protocol\",  # In this case all are from the same batch, but providing a batch key can improve batch integration by negative sampling\n",
    "    )\n",
    "    ds = constructor.get_dataset()\n",
    "    ds_dict[split_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"adata_link\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70612269",
   "metadata": {},
   "source": [
    "## Configure the Model\n",
    "The custom sentence transformers model will allow training with the sentence transformers Trainer. The numeric data has to be registered with the model, such that the representations in cell_sentence_1 (cell token eg. sample indices) are linked to the respective numeric vector, which serves as the initial repesentation of that sample. \n",
    "This is achieved by building a lookup table (cell token -> id) and a frozen embedding layer (id --> numeric vector). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from mmcontext.mmcontextencoder import MMContextEncoder\n",
    "\n",
    "enc = MMContextEncoder(\n",
    "    text_encoder_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    adapter_hidden_dim=128,\n",
    "    adapter_output_dim=64,\n",
    "    freeze_text_encoder=True,\n",
    "    unfreeze_last_n_layers=2,\n",
    "    output_token_embeddings=False,\n",
    "    train_lookup=False,\n",
    "    joint_adapter_hidden_dim=None,\n",
    "    text_model_kwargs=None,\n",
    "    use_text_adapter=True,\n",
    ")\n",
    "model = SentenceTransformer(modules=[enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df, _ = model[0].get_initial_embeddings_from_adata_link(\n",
    "    ds_dict,\n",
    "    layer_key=\"X_prot\",\n",
    "    download_dir=\"data_cache\",\n",
    "    axis=\"obs\",  # since we get embeddings from adata.obsm. We could also use \"varm\" and for example use an embedding for each protein\n",
    ")\n",
    "model[0].register_initial_embeddings(token_df, data_origin=\"prot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model expects a certain prefix on the cell tokens.\n",
    "model[0].processor.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc91911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could add this manually or use the method below\n",
    "model[0].prefix_ds(ds_dict, columns_to_prefix=[\"cell_sentence_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly we have to drop some columns from the dataset and\n",
    "# rename the main column to \"anchor\".\n",
    "# you might think that this is a bit cumbersome, which it is.\n",
    "# But this setup allowed for fleixble training,\n",
    "# using either cell or feature level tokens, using text based cell sentences\n",
    "# or numeric embeddings and resolving negatives\n",
    "# to whatever column was chosen for training.\n",
    "# That means that for a certain training run, the same dataset can be reused, and\n",
    "# only modified differently. But in the end, it is a bit of work to set up.\n",
    "ds_final = ds_dict.rename_column(\"cell_sentence_1\", \"anchor\")\n",
    "ds_final = ds_final.remove_columns([\"sample_idx\", \"adata_link\", \"negative_1_idx\"])\n",
    "ds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=10,\n",
    "    run_name=\"protein_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "evaluator = TripletEvaluator(\n",
    "    anchors=ds_final[\"val\"][\"anchor\"],\n",
    "    positives=ds_final[\"val\"][\"positive\"],\n",
    "    negatives=ds_final[\"val\"][\"negative_1\"],\n",
    "    name=\"val\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d32063",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_final[\"train\"],\n",
    "    eval_dataset=ds_final[\"val\"],\n",
    "    loss=loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f83d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
